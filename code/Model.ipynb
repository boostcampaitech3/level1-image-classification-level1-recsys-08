{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5c671f",
   "metadata": {},
   "source": [
    "## Model 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6866738",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from Dataset import ImageDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a579f",
   "metadata": {},
   "source": [
    "### Special Mission 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channel, shrink_channel):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, shrink_channel, (1, 1), padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(shrink_channel, in_channel, (3, 3), bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "              \n",
    "    def forward(self, x):\n",
    "        x_ = self.conv1(x)\n",
    "        x_ = self.relu(x_)\n",
    "        x_ = self.conv2(x_)\n",
    "        \n",
    "        x = x + x_\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, (3, 3), padding=1)\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(32, 64, (3, 3), stride=2, padding=1, bias=False),\n",
    "                                     nn.ReLU(),\n",
    "                                     ResidualBlock(64, 32)\n",
    "                                    )\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64, 128, (3, 3), stride=2, padding=1, bias=False),\n",
    "                                     nn.ReLU(),\n",
    "                                     ResidualBlock(128, 64),\n",
    "                                     ResidualBlock(128, 64)\n",
    "                                    )\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(128, 256, (3, 3), stride=2, padding=1, bias=False),\n",
    "                                     nn.ReLU(),\n",
    "                                     ResidualBlock(256, 128),\n",
    "                                     ResidualBlock(256, 128),\n",
    "                                     ResidualBlock(256, 128),\n",
    "                                     ResidualBlock(256, 128),\n",
    "                                     ResidualBlock(256, 128),\n",
    "                                     ResidualBlock(256, 128),\n",
    "                                     ResidualBlock(256, 128),\n",
    "                                     ResidualBlock(256, 128)\n",
    "                                    )\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(256, 512, (3, 3), stride=2, padding=1, bias=False),\n",
    "                                     nn.ReLU(),\n",
    "                                     ResidualBlock(512, 256),\n",
    "                                     ResidualBlock(512, 256),\n",
    "                                     ResidualBlock(512, 256),\n",
    "                                     ResidualBlock(512, 256),\n",
    "                                     ResidualBlock(512, 256),\n",
    "                                     ResidualBlock(512, 256),\n",
    "                                     ResidualBlock(512, 256),\n",
    "                                     ResidualBlock(512, 256)\n",
    "                                    )\n",
    "        self.layer5 = nn.Sequential(nn.Conv2d(512, 1024, (3, 3), stride=2, padding=1, bias=False),\n",
    "                                     nn.ReLU(),\n",
    "                                     ResidualBlock(1024, 512),\n",
    "                                     ResidualBlock(1024, 512),\n",
    "                                     ResidualBlock(1024, 512),\n",
    "                                     ResidualBlock(1024, 512)\n",
    "                                    )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(1024, 18)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abb9d7-3999-4737-ab6e-4434ff7cddcb",
   "metadata": {},
   "source": [
    "### Trainer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa71df-a8c3-492b-b83f-733aec40a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, optimizer, dataset, val_ratio, val_transform=None):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.dataset = dataset\n",
    "        self.data_cnt = len(self.dataset)\n",
    "        self.val_ratio = val_ratio\n",
    "        self.val_transform = val_transform\n",
    "        self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model = self.model.to(self.device)  \n",
    "    \n",
    "    def split_dataset(self, val_ratio):\n",
    "        data_cnt = len(self.dataset)\n",
    "        val_cnt = int(data_cnt * val_ratio)\n",
    "        train_cnt = data_cnt - val_cnt\n",
    "        train_set, val_set = random_split(self.dataset, [train_cnt, val_cnt])\n",
    "        if self.val_transform:\n",
    "            val_set.transform = self.val_transform\n",
    "        return train_set, val_set\n",
    "    \n",
    "    def generate_dataloaders(self, batch_size, shuffle=True, drop_last=False):\n",
    "        train_dataset, val_dataset = self.split_dataset(0.2)\n",
    "        train_dataloader = DataLoader(train_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      shuffle=shuffle,\n",
    "                                      drop_last=drop_last\n",
    "                                     )\n",
    "        val_dataloader = DataLoader(val_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      shuffle=shuffle,\n",
    "                                      drop_last=drop_last\n",
    "                                     )\n",
    "        return {'train' : train_dataloader,\n",
    "                'test' : val_dataloader }\n",
    "    \n",
    "    def step(self, epoch_size, batch_size):\n",
    "        dataloaders = self.generate_dataloaders(batch_size)\n",
    "        \n",
    "        train_acc_list = []\n",
    "        train_loss_list = []\n",
    "        test_acc_list = []\n",
    "        test_loss_list = []\n",
    "        \n",
    "        for epoch in range(epoch_size):\n",
    "            print(f'{epoch} epoch start({epoch+1}/{epoch_size})')\n",
    "            \n",
    "            loss, acc = self.run(batch_size, dataloaders['train'], 'train')\n",
    "            train_loss_list.append(loss)\n",
    "            train_acc_list.append(acc)\n",
    "            \n",
    "            loss, acc = self.run(batch_size, dataloaders['test'], 'test')\n",
    "            test_loss_list.append(loss)\n",
    "            test_acc_list.append(acc)\n",
    "            \n",
    "            print(f'{epoch} epoch done({epoch+1}/{epoch_size})')\n",
    "        \n",
    "        return [train_loss_list, test_loss_list], [train_acc_list, test_acc_list]\n",
    "    \n",
    "    def run(self, batch_size, dataloader, mode='train'):\n",
    "        loss, acc = 0, 0\n",
    "        \n",
    "        if mode == 'train': self.model.train()\n",
    "        elif mode == 'test': self.model.eval()\n",
    "        \n",
    "        for data in (tbar := tqdm(dataloader)):\n",
    "            X, y = data\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(mode == 'train'):\n",
    "                label = self.model(X)\n",
    "                _, predict = torch.max(label, 1)\n",
    "                loss = self.loss_fn(label, y)\n",
    "\n",
    "                if mode == 'train':\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                            \n",
    "            loss_mean = loss.item()\n",
    "            loss += loss_mean\n",
    "            correct_num = torch.sum(predict == y.data).item()\n",
    "            acc += correct_num / X.shape[0]\n",
    "            tbar.set_postfix({'loss': loss_mean, 'acc': correct_num})\n",
    "        \n",
    "        loss = loss / len(dataloader)\n",
    "        acc = acc / len(dataloader)\n",
    "\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b72c3ee-86b0-4c58-913e-984c545e1d83",
   "metadata": {},
   "source": [
    "### ResNet18 pretrained model 활용\n",
    "fc layer의 out_feature를 18로 수정했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8218f1-85f1-4324-aa49-db297716c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_pretrained = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18_pretrained.fc = nn.Linear(512, 18, bias=True)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(resnet18_pretrained.fc.weight)\n",
    "stdv = 1./math.sqrt(resnet18_pretrained.fc.weight.size(1))\n",
    "resnet18_pretrained.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "# freeze\n",
    "for name, param in resnet18_pretrained.named_parameters():\n",
    "    if name in ['fc.weight', 'fc.bias']:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddb9fb-0ac4-4eb8-aa8e-c8ba4a75baeb",
   "metadata": {},
   "source": [
    "### Densenet161 pretrained model 활용\n",
    "classifier의 out_feature를 18로 수정했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15777221-704d-4267-8919-744b9d5dfe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_pretrained = torchvision.models.densenet161(pretrained=True)\n",
    "\n",
    "densenet_pretrained.classifier = nn.Linear(2208, 18, bias=True)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(densenet_pretrained.classifier.weight)\n",
    "stdv = 1./math.sqrt(densenet_pretrained.classifier.weight.size(1))\n",
    "densenet_pretrained.classifier.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "# freeze\n",
    "for name, param in densenet_pretrained.named_parameters():\n",
    "    if name in ['classifier.weight', 'classifier.bias']:\n",
    "        param.require_grad = True\n",
    "    else:\n",
    "        param.require_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3238b36-d7ce-48fe-b5a4-dc9bbfd7cb15",
   "metadata": {},
   "source": [
    "### VGG16 pretrained model 활용\n",
    "classifier 2번째 linear layer의 out_feature를 2048, 3번째 linear layer의 in features, out_feature를 각각 2048, 18로 수정했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c4339-49ae-46cc-aa2a-6ea709f6c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vggnet_pretrained = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "vggnet_pretrained.classifier[3] = nn.Linear(4096, 2048, bias=True)\n",
    "vggnet_pretrained.classifier[6] = nn.Linear(2048, 18, bias=True)\n",
    "\n",
    "for i in [3, 6]:\n",
    "    torch.nn.init.xavier_uniform_(vggnet_pretrained.classifier[i].weight)\n",
    "    stdv = 1./math.sqrt(vggnet_pretrained.classifier[i].weight.size(1))\n",
    "    vggnet_pretrained.classifier[i].bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "# freeze\n",
    "for name, param in densenet_pretrained.named_parameters():\n",
    "    if name.startswith('classifier'):\n",
    "        param.require_grad = True\n",
    "    else:\n",
    "        param.require_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0589a0-161b-4c79-b654-e2a8ecdadde7",
   "metadata": {},
   "source": [
    "### 모델 학습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8e3eb-325d-4cec-b5c7-0a38426c6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/data/train'\n",
    "\n",
    "val_transform = transforms.Compose([transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.CenterCrop((384, 256)),\n",
    "                                                transforms.Normalize(0.5, 0.2)])\n",
    "\n",
    "image_dataset = ImageDataset(data_dir, 'new_train.csv', \n",
    "                             transform=transforms.Compose([transforms.RandomRotation(degrees = 15),\n",
    "                                                           transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
    "                                                           transforms.RandomVerticalFlip(p=0.5),\n",
    "                                                           transforms.ToTensor(),\n",
    "                                                           transforms.CenterCrop((384, 256)),\n",
    "                                                           transforms.Normalize(0.5, 0.2)])\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca7454-eb08-4c0b-b041-a734c34f0c1d",
   "metadata": {},
   "source": [
    "#### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7677dc-d7c0-4d5e-a79b-270058b7886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(resnet18_pretrained.parameters(), lr=learning_rate)\n",
    "\n",
    "# hyper-parameters\n",
    "num_epoch = 5\n",
    "num_batch = 32\n",
    "\n",
    "resnet_trainer = Trainer(resnet18_pretrained, loss_fn, \n",
    "                         optimizer, image_dataset, 0.2, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae6f99-72ee-40cc-a857-ce7a5bf0782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = resnet_trainer.step(num_epoch, num_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2c13c-280a-483f-a763-0c355a5bee37",
   "metadata": {},
   "source": [
    "#### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abafdd3-86ba-42e4-8df7-8aa73cb3f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(densenet_pretrained.parameters(), lr=learning_rate)\n",
    "\n",
    "# hyper-parameters\n",
    "num_epoch = 5\n",
    "num_batch = 32\n",
    "\n",
    "model_trainer = Trainer(densenet_pretrained, loss_fn, optimizer, image_dataset, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4e11d-fd55-486f-b5a1-79d1c0bf3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = model_trainer.step(num_epoch, num_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc04ba-aef4-4c83-b4df-e01e4fd65b4c",
   "metadata": {},
   "source": [
    "#### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6c8db-2c41-4076-a641-b98d51533804",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(vggnet_pretrained.parameters(), lr=learning_rate)\n",
    "\n",
    "# hyper-parameters\n",
    "num_epoch = 5\n",
    "num_batch = 16\n",
    "\n",
    "model_trainer = Trainer(vggnet_pretrained, loss_fn, optimizer, image_dataset, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbfa2c1-4edc-4063-b2fd-8fe5270c0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = model_trainer.step(num_epoch, num_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73161b6a-f6df-4c63-9ba1-9424ac2d893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vggnet_pretrained, './vgg.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a442dd5-13f1-4bf3-b594-7043a23ae601",
   "metadata": {},
   "source": [
    "### eval Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cd745-d0a3-4aa7-8616-e747c3d77f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    '''\n",
    "    csv 데이터를 통해 만들어진 Dataset Class\n",
    "    input: base(string)\n",
    "           filename(string)\n",
    "           transform(torchvision.transforms.transforms, default=None)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, base, filename, transform) -> None:\n",
    "        self.path = base\n",
    "        self.data = pd.read_csv(self.path+'/'+filename)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        X = np.array(Image.open(self.path+'/images/'+self.data['ImageID'][idx]))\n",
    "        \n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "            \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f7667-1c38-4354-bb7d-71bf94117846",
   "metadata": {},
   "source": [
    "### eval data를 이용해 예측해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a29adf-dc8b-403f-bc28-ab517c670d02",
   "metadata": {},
   "source": [
    "#### eval/info.csv 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e9685-72bd-49c5-a78e-5f581ab42cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = '../input/data/eval'\n",
    "filename = 'info.csv'\n",
    "\n",
    "test_info = pd.read_csv(eval_dir+'/info.csv')\n",
    "test_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e893b-f287-4a69-924d-54e15b0a6107",
   "metadata": {},
   "source": [
    "#### dataset, dataloader 생성 및 eval 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8cc77e-f5bd-476b-b36a-959fbc0877e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "eval_dir = '../input/data/eval'\n",
    "filename = 'info.csv'\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "eval_dataset = EvalDataset(eval_dir, filename, transform=transforms.Compose([\n",
    "                                                                transforms.ToTensor(),\n",
    "                                                                transforms.CenterCrop((384, 256)),\n",
    "                                                                transforms.Normalize(0.5, 0.2)])\n",
    "                                                            )\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "def eval_model(model, dataloader, device, batch_size, dataframe):\n",
    "    model.eval()\n",
    "    for idx, data in enumerate(tbar := tqdm(dataloader)):\n",
    "        X = data\n",
    "        X = X.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            label = model(X)\n",
    "            _, predict = torch.max(label, 1)\n",
    "        dataframe['ans'].iloc[idx*batch_size:idx*batch_size+predict.shape[0]] = predict.tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d0aa9-835f-49c8-aadd-69b26e713b89",
   "metadata": {},
   "source": [
    "vgg16 모델을 이용해 예측해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2e925-b7e3-41e4-8285-94a7209d2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(vggnet_pretrained, eval_dataloader, device, batch_size, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e04cdc-cde1-44ba-afca-0e28ea8b4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info.to_csv('vgg16base(5, 16, Adam, 1e-4)', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e84a82-5ef4-4ca7-83ef-c2545ac5be47",
   "metadata": {},
   "source": [
    "#### 결과 시각화\n",
    "임의의 N개의 결과에 대해 시각화해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad71c01-3c97-4586-9d54-83542f11e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 21\n",
    "\n",
    "fig, ax = plt.subplots(N//3, 3, figsize=(3*3, N))\n",
    "idxs = np.random.choice(len(test_info), N)\n",
    "ax = ax.flatten()\n",
    "for idx, data_idx in enumerate(idxs):\n",
    "    pred = test_info['ans'].iloc[data_idx]\n",
    "    image_path = f\"{eval_dir}/images/{test_info['ImageID'].iloc[data_idx]}\"\n",
    "    image = plt.imread(image_path)\n",
    "\n",
    "    ax[idx].set_title(f'pred: {pred}')\n",
    "    ax[idx].imshow(image)\n",
    "    ax[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48084d-e155-4a0a-a0a2-38f56fc82452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1d0cb667-9868-4f8c-b43b-57928294a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "#import torchtext\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ced31980-2cb2-4db1-9c3b-947a80379536",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/opt/ml/input/data/train/\"\n",
    "train_image_dir_path = os.path.join(train_path, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "967bb457-15ca-4ed6-99fd-644cf7315492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d4efd345-99c2-46b2-bc49-ae454c93074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(dirname, result): #하위 목록의 모든 파일을 찾는 함수\n",
    "    try:\n",
    "        filenames = os.listdir(dirname)\n",
    "        for filename in filenames:\n",
    "            if filename[0] == '.': #.으로 시작하는 애들 거름\n",
    "                continue\n",
    "            full_filename = os.path.join(dirname, filename)\n",
    "            if os.path.isdir(full_filename):\n",
    "                search(full_filename, result)\n",
    "            else:\n",
    "                ext = os.path.splitext(full_filename)[-1]# 확장자 체크\n",
    "                if ext:\n",
    "                    result.append(full_filename)\n",
    "    except PermissionError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d6b8871d-bebe-4e04-a57c-7c26d6c781a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path = []\n",
    "search(train_image_dir_path, all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "96e35451-d806-4cb6-b974-acd33ecf6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터의 디렉토리는 2700개."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "19aeebee-e662-4b1d-af6e-dcf043e01ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18900"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_path) #2700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e26139d6-2169-4422-9da5-312eb5f7fb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/ml/input/data/train/images/006451_female_Asian_18/mask3.jpg',\n",
       " '/opt/ml/input/data/train/images/006451_female_Asian_18/normal.jpg',\n",
       " '/opt/ml/input/data/train/images/006451_female_Asian_18/mask5.jpg',\n",
       " '/opt/ml/input/data/train/images/006451_female_Asian_18/mask4.jpg',\n",
       " '/opt/ml/input/data/train/images/006451_female_Asian_18/incorrect_mask.jpg',\n",
       " '/opt/ml/input/data/train/images/006451_female_Asian_18/mask2.jpg',\n",
       " '/opt/ml/input/data/train/images/006451_female_Asian_18/mask1.jpg',\n",
       " '/opt/ml/input/data/train/images/004241_male_Asian_60/mask3.jpg',\n",
       " '/opt/ml/input/data/train/images/004241_male_Asian_60/normal.jpg',\n",
       " '/opt/ml/input/data/train/images/004241_male_Asian_60/mask5.jpg']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_path[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4efb4c6a-3834-47a1-b299-0ac7042b7fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.jpg', '.jpeg', '.png']\n"
     ]
    }
   ],
   "source": [
    "exts = []\n",
    "for i in all_path:\n",
    "    ext = os.path.splitext(i)[-1]\n",
    "    if ext not in exts:\n",
    "        exts.append(ext)\n",
    "print(exts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "70f5519e-989d-4063-a695-98bd82b428c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path = sorted(all_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a82cff5c-e3ca-435a-a7dc-da3d30a94c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#라벨링 하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "31cc4bb9-fc69-4114-b008-14d335e9b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(name):\n",
    "    label = 0\n",
    "    info, mask_type = name.split('/')[-2:]\n",
    "    info = info.split('_')\n",
    "    gender, age = info[1], int(info[3])\n",
    "    if 'incorrect' in mask_type:\n",
    "        label += 6\n",
    "    elif 'normal' in mask_type:\n",
    "        label += 12\n",
    "    \n",
    "    if gender == 'female':\n",
    "        label += 3\n",
    "    \n",
    "    if 30<= age < 60:\n",
    "        label += 1\n",
    "    elif age >= 60:\n",
    "        label += 2\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "29cbf8f0-32f2-491b-ad1a-fedb027654c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path, label을 컬럼으로 갖는 dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b214afdb-e951-467c-9778-6bebb78c532b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  label\n",
       "0      /opt/ml/input/data/train/images/000001_female_...     10\n",
       "1      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "2      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "3      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "4      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "...                                                  ...    ...\n",
       "18895  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18896  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18897  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18898  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18899  /opt/ml/input/data/train/images/006959_male_As...     12\n",
       "\n",
       "[18900 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path_label_df = pd.DataFrame(all_path, columns = ['path'])\n",
    "\n",
    "train_path_label['label'] = train_path_label_df['path'].map(lambda x : labeling(x))\n",
    "train_path_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e8fb207a-2a2f-45c3-9a4d-8836883e15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path_label.to_csv('./train_path_label.cvs', index=False, encoding='utf-8')\n",
    "#train_path_label = pd.read_csv('./train_path_label.csv, econding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f5ae58a-9e48-411d-b9e0-2ee185265bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset을 상속받은 CustomDataset\n",
    "# transform 은 size를 512 384로 변형, tensor, 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "faf99d2b-25c2-47b8-b0d5-ba93b33a8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths_label, transform):\n",
    "        self.X = img_paths_label['path']\n",
    "        self.Y = img_paths_label['label']\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X.iloc[index])\n",
    "        label = self.Y.iloc[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "66f10af3-8e56-4da4-8320-415e85d621e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d5ad2920-9dc4-4f4d-8d56-9ec4b2b45e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, valid를 나누는 부분\n",
    "#label의 비율을 유지하면서 나눔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3e9b1f72-5e15-4782-9fc6-698e1bc6f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(train_path_label, test_size=0.2, \n",
    "                               shuffle = True, stratify=train_path_label['label'], \n",
    "                               random_state = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2cfb237e-e61d-4ddf-92ef-cc06935860ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15120, 2), (3780, 2))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9995e91d-2d03-4f71-8624-b417212e97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "150e0168-d1fa-465a-8a49-9a380afaa49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "91ffcca8-bd7b-481b-9823-d943f0af14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = CustomDataset(valid, transform)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, \n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1957d27f-8c95-44fb-bcc6-c0f4add5390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader는 (batch_size, channell, height, wide)를 출력해줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dcc4ac59-c828-40ef-b31c-5d367c1cd910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 512, 384])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c7e1520a-e3d8-46bf-82f7-c0cec1b55b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델\n",
    "# pretrained 된 resnet18사용\n",
    "#마지막 fc층만 18개의 class로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "acf2fb78-10c6-45a0-a279-66f7f96b042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8e3368b8-c211-4b84-8388-78c31643689c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "OUTPUT_CLASS_NUM = 18\n",
    "resnet18.fc = torch.nn.Linear(in_features = 512, out_features= OUTPUT_CLASS_NUM, bias = True) # num of ouput = 18\n",
    "\n",
    "# xavier uniform\n",
    "torch.nn.init.xavier_uniform_(resnet18.fc.weight)\n",
    "stdv = 1. / math.sqrt(resnet18.fc.weight.size(1))\n",
    "resnet18.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "resnet18.fc.weight.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4aa94f2e-27b4-4dad-8924-4e8b32245af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "06bd3c08-e03f-4088-a482-3ce77b85255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch=5, lr=0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9e59c0d0-8f9f-4e57-97f2-7b1bc65a37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 5\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\" : train_dataloader,\n",
    "    \"test\" : valid_dataloader\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93479a5e-13d7-420d-85be-ab449d51b8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.022, 평균 Accuracy : 0.994\n",
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss : 0.064, 평균 Accuracy : 0.981\n"
     ]
    }
   ],
   "source": [
    "best_test_accuracy = 0.\n",
    "best_test_loss = 9999.\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        if phase == \"train\":\n",
    "            resnet18.train() # 네트워크 모델을 train 모드로 두어 gradient을 계산하고, 여러 sub module (배치 정규화, 드롭아웃 등)이 train mode로 작동할 수 있도록 함\n",
    "        elif phase == \"test\":\n",
    "            resnet18.eval() # 네트워크 모델을 eval 모드로 두어 여러 sub module들이 eval mode로 작동할 수 있게 함\n",
    "        \n",
    "        for ind, (images, labels) in enumerate(dataloaders[phase]):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # parameter gradient를 업데이트 전 초기화함\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"): # train 모드일 시에는 gradient를 계산하고, 아닐 때는 gradient를 계산하지 않아 연산량 최소화\n",
    "                logits = resnet18(images)\n",
    "                _, preds = torch.max(logits, 1) # 모델에서 linear 값으로 나오는 예측 값 ([0.9, 1.2, 3.2, 0.1, -0.1,...])을 최대 ouput index를 찾아 예측 레이블 ([2])로 변경함\n",
    "                loss = loss_fn(logits, labels)\n",
    "                \n",
    "                if phase ==\"train\":\n",
    "                    loss.backward() # 모델의 예측 값과 실제 값의 CrossEntrophy 차이를 통해 gradient 계산\n",
    "                    optimizer.step() # 계산된 gradient를 가지고 모델 업데이트\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "            running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "        \n",
    "        # 한 epoch이 모두 종료되었을 때,\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "        print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}\")\n",
    "        if phase == \"test\" and best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "            best_test_accuracy = epoch_acc\n",
    "        if phase == \"test\" and best_test_loss > epoch_loss: # phase가 test일 떄, best loss 계산\n",
    "            best_test_loss = epoch_loss\n",
    "print(\"학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss : {best_test_loss}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534a6d0-1f0d-4062-931b-20026c281d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_eval(model, data_iter, device):\n",
    "    with torch.no_grad():\n",
    "        n_total, n_correct = 0, 0\n",
    "        model.eval()\n",
    "        for batch_in, batch_out in data_iter:\n",
    "            y_trgt = batch_out.to(device)\n",
    "            model_pred = model.forward(batch_in.to(device))\n",
    "            _, y_pred = torch.max(model_pred, 1) # 행으로 비교\n",
    "            n_correct += (y_pred == y_trgt).sum().item()\n",
    "            n_total += batch_in.size(0)\n",
    "        val_accr = (n_correct/n_total)\n",
    "        #model.train()\n",
    "    return val_accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b1188-7f42-4164-9d95-6cd40c4b1c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(func_eval(resnet18, valid_dataloader, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5068bab5-18ed-4e92-a6fe-07967b6ac3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_eval(raw_data, dataloader, model, device):\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            model_pred = model.forward(X.to(device))\n",
    "            _, y_pred = torch.max(model_pred, 1)\n",
    "            \n",
    "            result.append([valid.iloc[i]['path'], y_pred.cpu().numpy()[0], y.cpu().numpy()[0]])\n",
    "    result = pd.DataFrame(result, columns=['path', 'pred', 'target'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b17d8-d21b-47eb-a191-de0441966d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_testing_dataloader = DataLoader(valid_dataset, shuffle = False)\n",
    "check_eval_df = check_eval(valid, valid_testing_dataloader, resnet18, device)\n",
    "check_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c339c02-bf2d-40c7-b5dd-53e9b80326b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df = check_eval_df[check_eval_df['pred'] != check_eval_df['target']]\n",
    "wrong_df = wrong_df.reset_index(drop=True)\n",
    "wrong_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0504a6da-90d4-4d01-8cc3-a66c04155b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def draw_(df):\n",
    "    plt.figure(figsize = (15,30))\n",
    "    row = 7 # wrong df의 개수에 따라 조정할 것\n",
    "    for i in range(df.shape[0]):\n",
    "        plt.subplot(row + 1, df.shape[0]//row, i+1)\n",
    "        plt.imshow(Image.open(df['path'][i]))\n",
    "        plt.title(f\"target:{df['target'][i]}, pred:{df['pred'][i]}\", color = 'r', size=20)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1a1aa-e983-4363-8869-d6ddc5bf3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_(wrong_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac20aeb-e7ae-4422-98bf-11d34b235929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
